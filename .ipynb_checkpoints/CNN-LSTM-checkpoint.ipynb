{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 1, 95)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build character map function for encoding URL string\n",
    "\n",
    "import string\n",
    "ascii_letters = string.ascii_letters # 1~52\n",
    "digits = string.digits # 53~62\n",
    "punctuation = string.punctuation # 63~94\n",
    "total_char = ascii_letters + digits + punctuation    \n",
    "\n",
    "UNKNOWN_CHAR = len(total_char) + 1\n",
    "TOTAL_FEATURES = UNKNOWN_CHAR + 1 # include the default padding integer 0 \n",
    "charmap = {\n",
    "    c: idx+1\n",
    "    for idx, c in enumerate(total_char)\n",
    "}\n",
    "\n",
    "def encodeChar(c):\n",
    "    return charmap.get(c, UNKNOWN_CHAR)\n",
    "\n",
    "encodeChar(\"x\"), encodeChar(\"a\"), encodeChar(\"æˆ‘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "import pandas\n",
    "import statistics\n",
    "df = pandas.read_csv(\"all_urls.csv\")\n",
    "\n",
    "df[\"len\"] = df.url.apply(lambda s: len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    23007.000000\n",
       "mean       116.103099\n",
       "std        114.441389\n",
       "min         15.000000\n",
       "25%         45.000000\n",
       "50%         79.000000\n",
       "75%        140.000000\n",
       "max       1641.000000\n",
       "Name: len, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the length stats\n",
    "df.len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=200 18.16404%\n",
      "x=300 4.48994%\n",
      "x=400 2.16891%\n",
      "x=500 1.51693%\n",
      "x=600 1.19094%\n",
      "x=700 0.52593%\n",
      "x=800 0.25644%\n",
      "x=900 0.19559%\n",
      "x=1000 0.10432%\n"
     ]
    }
   ],
   "source": [
    "# find a Length for large coverage for all sample URL\n",
    "# Length = 400 has ~98% coverage\n",
    "for t in [200, 300, 400, 500, 600, 700, 800, 900, 1000]:\n",
    "    print(\"x={} {:.5f}%\".format(t, 100 * sum(df.len.apply(lambda x: x > t)) / len(df.len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18405 4602 23007\n",
      "Loading data...\n",
      "11043 train sequences\n",
      "3681 test sequences\n",
      "3681 val sequences\n"
     ]
    }
   ],
   "source": [
    "# sampling train/test dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "sub_df, preserved_df = train_test_split(df, test_size=0.2, random_state=1)\n",
    "print(len(sub_df), len(preserved_df), len(df))\n",
    "# sub_df = df.sample(1000)\n",
    "categorical_label = np_utils.to_categorical(sub_df.label)\n",
    "\n",
    "url_train, url_test, y_train, y_test \\\n",
    "    = train_test_split(sub_df.url, categorical_label, test_size=0.2, random_state=1)\n",
    "\n",
    "url_train, url_val, y_train, y_val \\\n",
    "    = train_test_split(url_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "print('Loading data...')\n",
    "print(len(url_train), 'train sequences')\n",
    "print(len(url_test), 'test sequences')\n",
    "print(len(url_val), 'val sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some configurations \n",
    "\n",
    "# Embedding\n",
    "max_features = TOTAL_FEATURES\n",
    "maxlen = 400 # ~98% coverage, paper uses 96% coverage\n",
    "embedding_size = 128\n",
    "\n",
    "# Training\n",
    "batch_size = 64 # paper param\n",
    "epochs = 20 # paper param\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 2\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Dropout ratio\n",
    "Dropout_ratio = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (11043, 400)\n",
      "x_test shape: (3681, 400)\n",
      "x_val shape: (3681, 400)\n"
     ]
    }
   ],
   "source": [
    "# encode the URL by one-hot encoding and padding feature vector by 'pre'\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "\n",
    "x_train = pad_sequences(url_train.apply(lambda url: numpy.array([encodeChar(c) for c in url])), \n",
    "              maxlen=maxlen, \n",
    "              padding='pre')\n",
    "x_test = pad_sequences(url_test.apply(lambda url: numpy.array([encodeChar(c) for c in url])), \n",
    "              maxlen=maxlen, \n",
    "              padding='pre')\n",
    "\n",
    "x_val = pad_sequences(url_val.apply(lambda url: numpy.array([encodeChar(c) for c in url])), \n",
    "              maxlen=maxlen, \n",
    "              padding='pre')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('x_val shape:', x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"CNN-LSTM for phishing detection\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 400, 128)          12288     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 396, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 198, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 70)                37800     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 142       \n",
      "=================================================================\n",
      "Total params: 91,254\n",
      "Trainable params: 91,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential(name=\"CNN-LSTM for phishing detection\")\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen, trainable=True))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dropout(Dropout_ratio))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxcian/dev/python3/env/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11043 samples, validate on 3681 samples\n",
      "Epoch 1/20\n",
      "11043/11043 [==============================] - 38s 3ms/step - loss: 0.3532 - accuracy: 0.8460 - val_loss: 0.2888 - val_accuracy: 0.8883\n",
      "Epoch 2/20\n",
      "11043/11043 [==============================] - 42s 4ms/step - loss: 0.2054 - accuracy: 0.9270 - val_loss: 0.1685 - val_accuracy: 0.9435\n",
      "Epoch 3/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.1900 - accuracy: 0.9303 - val_loss: 0.1515 - val_accuracy: 0.9500\n",
      "Epoch 4/20\n",
      "11043/11043 [==============================] - 44s 4ms/step - loss: 0.1204 - accuracy: 0.9565 - val_loss: 0.1113 - val_accuracy: 0.9606\n",
      "Epoch 5/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.1064 - accuracy: 0.9617 - val_loss: 0.1206 - val_accuracy: 0.9573\n",
      "Epoch 6/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.0883 - accuracy: 0.9697 - val_loss: 0.1062 - val_accuracy: 0.9628\n",
      "Epoch 7/20\n",
      "11043/11043 [==============================] - 44s 4ms/step - loss: 0.0727 - accuracy: 0.9751 - val_loss: 0.1052 - val_accuracy: 0.9663\n",
      "Epoch 8/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.0760 - accuracy: 0.9735 - val_loss: 0.2738 - val_accuracy: 0.9063\n",
      "Epoch 9/20\n",
      "11043/11043 [==============================] - 46s 4ms/step - loss: 0.0732 - accuracy: 0.9753 - val_loss: 0.0881 - val_accuracy: 0.9712\n",
      "Epoch 10/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.0427 - accuracy: 0.9854 - val_loss: 0.1043 - val_accuracy: 0.9644\n",
      "Epoch 11/20\n",
      "11043/11043 [==============================] - 45s 4ms/step - loss: 0.0477 - accuracy: 0.9843 - val_loss: 0.0910 - val_accuracy: 0.9728\n",
      "Epoch 12/20\n",
      "11043/11043 [==============================] - 44s 4ms/step - loss: 0.0391 - accuracy: 0.9872 - val_loss: 0.1009 - val_accuracy: 0.9674\n",
      "Epoch 13/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.0392 - accuracy: 0.9869 - val_loss: 0.0870 - val_accuracy: 0.9717\n",
      "Epoch 14/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.0332 - accuracy: 0.9884 - val_loss: 0.1269 - val_accuracy: 0.9669\n",
      "Epoch 15/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.0335 - accuracy: 0.9886 - val_loss: 0.0999 - val_accuracy: 0.9756\n",
      "Epoch 16/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.0174 - accuracy: 0.9938 - val_loss: 0.1119 - val_accuracy: 0.9723\n",
      "Epoch 17/20\n",
      "11043/11043 [==============================] - 46s 4ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.1097 - val_accuracy: 0.9750\n",
      "Epoch 18/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.0669 - accuracy: 0.9774 - val_loss: 0.1246 - val_accuracy: 0.9688\n",
      "Epoch 19/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.0203 - accuracy: 0.9934 - val_loss: 0.1020 - val_accuracy: 0.9720\n",
      "Epoch 20/20\n",
      "11043/11043 [==============================] - 43s 4ms/step - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.1102 - val_accuracy: 0.9747\n",
      "3681/3681 [==============================] - 4s 1ms/step\n",
      "Test score: 0.08318285195319364\n",
      "Test accuracy: 0.9798967838287354\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "#           epochs=3,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11043/11043 [==============================] - 9s 819us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0028931456909181076, 0.9995472431182861]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3681/3681 [==============================] - 3s 813us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11019592106831128, 0.9747351408004761]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4602/4602 [==============================] - 4s 834us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12141881303142098, 0.9734897613525391]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate preserved_df\n",
    "preserved_x = pad_sequences(preserved_df.url.apply(lambda url: numpy.array([encodeChar(c) for c in url])), \n",
    "              maxlen=maxlen, \n",
    "              padding='pre')\n",
    "\n",
    "preserved_y = np_utils.to_categorical(preserved_df.label)\n",
    "model.evaluate(preserved_x, preserved_y, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
